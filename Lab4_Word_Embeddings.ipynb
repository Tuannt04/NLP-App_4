{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c85db0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "!pip install gensim scikit-learn matplotlib seaborn pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc7e08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ThÃªm Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c src Ä‘á»ƒ import module\n",
    "sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from representations.word_embedder import WordEmbedder\n",
    "\n",
    "# Khá»Ÿi táº¡o embedder\n",
    "embedder = WordEmbedder(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# 1. Láº¥y vector cá»§a má»™t tá»«\n",
    "word = \"king\"\n",
    "vec = embedder.get_vector(word)\n",
    "print(f\"ğŸ”¹ Vector cá»§a '{word}' (10 pháº§n tá»­ Ä‘áº§u):\\n\", vec[:10])\n",
    "\n",
    "# 2. TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (similarity)\n",
    "sim_king_queen = embedder.get_similarity(\"king\", \"queen\")\n",
    "sim_king_man = embedder.get_similarity(\"king\", \"man\")\n",
    "print(f\"\\nğŸ‘‘ Similarity(king, queen) = {sim_king_queen:.4f}\")\n",
    "print(f\"ğŸ§ Similarity(king, man)   = {sim_king_man:.4f}\")\n",
    "\n",
    "# 3. TÃ¬m cÃ¡c tá»« Ä‘á»“ng nghÄ©a (most similar)\n",
    "print(\"\\nğŸ§  5 tá»« giá»‘ng 'computer' nháº¥t:\")\n",
    "for w, score in embedder.get_most_similar(\"computer\", top_n=5):\n",
    "    print(f\"  {w:<15} {score:.4f}\")\n",
    "\n",
    "# 4. NhÃºng má»™t vÄƒn báº£n\n",
    "doc = \"The queen rules the country.\"\n",
    "vec_doc = embedder.embed_document(doc)\n",
    "print(f\"\\nğŸ“„ Embedding cá»§a vÄƒn báº£n (10 pháº§n tá»­ Ä‘áº§u):\\n {vec_doc[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff619902",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cháº¡y script huáº¥n luyá»‡n (báº¡n cÃ³ thá»ƒ cháº¡y trá»±c tiáº¿p trong terminal hoáº·c tá»« notebook)\n",
    "# !python test/lab4_embedding_training_demo.py\n",
    "\n",
    "# Sau khi cháº¡y, ta cÃ³ thá»ƒ táº£i láº¡i model vÃ  sá»­ dá»¥ng\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "trained_model_path = \"results/word2vec_ewt.model\"\n",
    "if os.path.exists(trained_model_path):\n",
    "    trained_model = Word2Vec.load(trained_model_path)\n",
    "    print(\"âœ… Model tá»± huáº¥n luyá»‡n Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.\")\n",
    "    \n",
    "    # Thá»­ tÃ¬m tá»« Ä‘á»“ng nghÄ©a\n",
    "    print(\"\\nğŸ”¹ CÃ¡c tá»« giá»‘ng 'language' nháº¥t (tá»« model tá»± huáº¥n luyá»‡n):\")\n",
    "    similar_words = trained_model.wv.most_similar(\"language\", topn=5)\n",
    "    for w, score in similar_words:\n",
    "        print(f\"  {w:<15} {score:.4f}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ KhÃ´ng tÃ¬m tháº¥y model táº¡i {trained_model_path}. HÃ£y cháº¡y script training trÆ°á»›c.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0abc74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Danh sÃ¡ch cÃ¡c tá»« Ä‘á»ƒ trá»±c quan hÃ³a\n",
    "words = [\n",
    "    'king', 'queen', 'man', 'woman',\n",
    "    'france', 'paris', 'germany', 'berlin',\n",
    "    'apple', 'google', 'microsoft',\n",
    "    'dog', 'cat', 'pet'\n",
    "]\n",
    "\n",
    "# Láº¥y vector cho cÃ¡c tá»«\n",
    "word_vectors = np.array([embedder.get_vector(w) for w in words])\n",
    "\n",
    "# Giáº£m chiá»u báº±ng PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Váº½ biá»ƒu Ä‘á»“\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c='blue', edgecolors='k', alpha=0.6)\n",
    "\n",
    "# Ghi chÃº tÃªn cÃ¡c tá»«\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), fontsize=12)\n",
    "    \n",
    "plt.title(\"Trá»±c quan hÃ³a Word Embeddings báº±ng PCA\", fontsize=16)\n",
    "plt.xlabel(\"ThÃ nh pháº§n chÃ­nh 1\", fontsize=12)\n",
    "plt.ylabel(\"ThÃ nh pháº§n chÃ­nh 2\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
